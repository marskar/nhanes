---
author: 'Martin Skarzynski'
date: '`r Sys.Date()`'
institution: 'Johns Hopkins University'
division: 'School of Public Health'
advisor: 'Professor Elizabeth Platz'
department: 'Epidemiology'
degree: 'Master of Public Health'
title: 'Exploratory Analysis of Factors Associated with Cancer Mortality in the National Health and Nutrition Examination Survey Dataset'
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: 
  thesisdown::thesis_pdf: default
# thesisdown::thesis_gitbook: default
#  thesisdown::thesis_word: default
#  thesisdown::thesis_epub: default
# If you are creating a PDF you'll need to write your preliminary content here or
# use code similar to line 20 for the files.  If you are producing in a different
# format than PDF, you can delete or ignore lines 20-31 in this YAML header.
#abstract: |
#  `r if(knitr:::is_latex_output()) paste(readLines("00-abstract.Rmd"), collapse = '\n  ')`
# If you'd rather include the preliminary content in files instead of inline
# like below, use a command like that for the abstract above.  Note that a tab is 
# needed on the line after the |.
#acknowledgements: |
#  I want to thank a few people.
#dedication: |
#  You can have a dedication here if you wish. 
#preface: |
#  This is an example of a thesis setup to use the reed thesis document class
#  (for LaTeX) and the R bookdown package, in general.
bibliography: bib/thesis.bib
# Download your specific bibliography database file and refer to it in the line above.
csl: csl/biomed-central.csl
# Download your specific csl file and refer to it in the line above.
lot: false
lof: false
toc: false
space_between_paragraphs: true
#space_between_paragraphs: true
# Delete the # at the beginning of the previous line if you'd like
# to have a blank new line between each paragraph
#header-includes:
#- \usepackage{tikz}
---

<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd prefer to not include a Dedication, for example, simply delete lines 17 and 18 above or add a # before them to comment them out.  If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.
-->

<!--
If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r include_packages, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis.
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(thesisdown))
  devtools::install_github("ismayc/thesisdown")
library(thesisdown)
```

<!-- You'll need to include the order that you'd like Rmd files to appear in the _bookdown.yml file for
PDF files and also delete the # before rmd_files: there.  You'll want to not include 00(two-hyphens)prelim.Rmd
and 00-abstract.Rmd since they are handled in the YAML above differently for the PDF version.
-->

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers
on chapters.
-->


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE, warning = FALSE, message = FALSE)
```

```{r libraries, include=FALSE}
library(knitr)
library(readr)
```

## Abstract

**Context:** Large epidemiologic cohort studies, such as the National Health and Nutrition Examination Survey (NHANES), collect copious high-dimensional data that allow for examination of multiple exposures in relation to a given outcome.

**Objective:** To explore the exposures measured in the Third National Health and Nutrition Examination Survey (NHANES III) dataset in search of factors associated with cancer mortality data obtained from the National Death Index (NDI) and to assess methods for lethal cancer risk prediction model variable selection.

**Methods:** We fit thousands of Cox proportional hazards models with and without ridge penalties to randomly selected subsets of up to 50 variables. We analyzed the descriptions of NHANES variables provided by NCHS and selected 3 variables that are important, non-modifiable risk factors for cancer (age, sex and race/ethnicity) to include in all future models. We then compared variables that appeared most frequently in the Cox models with high significance (p < 10^-10^) and selected 5 high-frequency, highly significant variables that we used to train a new group of models with fewer randomized variables.

**Results:** The models with selected variables outperformed the fully randomized models in terms of concordance. Across all of the models, the ten variables that most frequently surpassed the p-value threshold were age, race/ethnicity, lifetime consumption of more than 100 cigarettes, 3 variables that pertain to physical activity and 3 variables that may be related to aging.

**Conclusions:** The work described here constitutes an exploratory analysis of the NHANES III dataset that employs an iterative strategy for the generation of cancer risk prediction models. Looking beyond this demonstration of a variable selection method, our ultimate goal is to build upon previously-described cancer risk factors towards the discovery of novel contributors to cancer risk, a deeper understanding of cancer etiology, and an improved ability to predict cancer incidence and mortality.

## Introduction

Cancer susceptibility is influenced by modifiable and non-modifiable factors. Modifiable cancer risk factors include body mass index (BMI) and cigarette use, whereas the non-modifiable factors include age, sex, race/ethnicity, single nucleotide polymorphisms (SNPs), and family history of disease. According to a 2018 study by Islami and colleagues [@Islami_2018], modifiable risk factors are responsible for 42% of all cancer cases and 45% of all cancer deaths. This finding suggests that cancer prevention strategies that target modifiable risk factors have the potential to almost halve cancer incidence and mortality in the United States. A near two-fold reduction in cancer cases and deaths may seem far-fetched, but cancer incidence and mortality in United Status have been declining by ~1.5% every year from 2009-2014 and 2001-2015, respectively [@Siegel_2018]. Taken together, these data indicate that while tremendous progress has been made, there is still great potential for cancer prevention approaches to decrease cancer incidence and mortality.

The scale of cancer burden in the United States is staggering. Siegel and colleagues estimate that in 2018 there will be 1.7 million newly diagnosed cancer cases and roughly 600 thousand cancer deaths [@Siegel_2018]. Cancer risk prediction models can help policymakers and cancer prevention practitioners develop more effective interventions and to channel limited resources towards people at the greatest risk. To achieve the best performance, cancer risk prediction models must include both modifiable and non-modifiable risk factors. In 2016, Maas and colleagues [@Maas_2016] demonstrated that cancer risk prediction models based on known epidemiologic risk factors can be improved when genetic information such as SNPs are included in the models. Importantly, the combined model provided better risk stratification than the models containing only epidemiologic risk factors or only genetic variables.

The challenge of cancer risk prediction is complex and will require cancer-type specific strategies that integrate multiple types of data and explore various modeling methods. In addition to deepening our understanding of known cancer risk factors, it is imperative to identify new factors that may only be meaningful in the larger context of contributors to cancer risk. This larger context includes the collection of genetic inheritance, called the genome, and the myriad exposures that individuals experience during their lives, known as the exposome [@Wild_2005].

Some genetic factors and environmental exposures may be very strongly linked to cancer. Examples of well-described genetic and environmental cancer risk factors include TP53 gene mutation in Li-Fraumeni Syndrome and asbestos inhalation in mesothelioma, respectively. One of the strongest cancer risk factors is cigarette smoking. In fact, smoking was the strongest modifiable risk factor in the 2018 study by Islami and colleagues [@Islami_2018]. In this study, Islami and colleagues determined that 19% of all cancers cases and roughly 29% of all cancers deaths can be attributed to cigarette smoking [@Islami_2018]. To look beyond known cancer risk factors like cigarette smoking, new cancer risk prediction models will need to detect small, but meaningful effects amid a sea of other variables.

As part of the effort to tackle this challenge, we analyzed data from [Third National Health and Nutrition Examination Survey (NHANES III)](https://wwwn.cdc.gov/nchs/nhanes/nhanes3/DataFiles.aspx) [@nhanes_1994] and the accompanying [National Death Index (NDI) Public-Use Linked Mortality Files](https://www.cdc.gov/nchs/data-linkage/mortality-public.htm). The first goal of the analysis was to explore the available NHANES III data and identify potential variables of interest for cancer mortality risk prediction. The second goal was to define an approach for variable selection for cancer risk prediction models. NHANES III is different from many other studies, in that instead of randomly sampling, NHANES utilizes a complex design that employs probability-based sampling in multiple stages [@nhanes_1994].

While the current work focuses solely on NHANES III, the data exploration and variable selection methods described here can potentially be applied to other studies, even those that have different designs. For example, the Atherosclerosis Risk in Communities (ARIC) study [@ARIC_1989] and the Framingham Heart Study (FHS) [@Mahmood_2014] are, like NHANES, large cohort studies that do not focus on cancer, but include relevant cancer outcomes as part of rich, multidimensional datasets. In fact, the ARIC [@Joshu_2017], NHANES [@Freedman_2010], and FHS [@Kreger_1991] datasets have already proven useful for cancer research.

## Methods

The Third National Health and Nutrition Examination Survey (NHANES III) collected data on 33,994 participants aged 2 months and older from 1988 to 1994 in the United States. The data, which include Interview, Medical Examination, and Laboratory components, were collected and linked with Mortality data from NDI death certificate records by the National Center for Health Statistics (NCHS) of the Centers for Disease Control and Prevention (CDC). From the initial pool of participants, we selected 16404 adult ($age \geq 18$) participants who were cancer-free at baseline and who had no missing values for follow-up time since interview, NDI mortality, primary sampling units (PSU), stratification, and sampling weight variables.

The initial publicly available dataset contained 3544 exposures from the Interview, Medical Examination, Laboratory, and Mortality components. After removing variables that were non-numeric, missing any values, only had one unique value, or had correlation to another variable greater than 0.9, we obtained the final set of 243 exposures. The analysis described here did not involve multiple imputation nor utilize the NHANES III Multiply Imputed Data Set. Among the 16404 participants, there were 964 cancer deaths and 280891 total years of follow-up since the initial Interview data were collected. The cancer deaths and follow-up time were used as the outcome (survival) in Cox proportional hazards regression analysis [@Therneau_2000].

NHANES III data and documentation are available on the [Centers for Disease Control (CDC) - National Center for Health Statistics (NCHS) website](https://wwwn.cdc.gov/nchs/nhanes/nhanes3/DataFiles.aspx). The National Death Index (NDI) linked mortality data are available separately on the [Public-Use Linked Mortality Files webpage](https://www.cdc.gov/nchs/data-linkage/mortality-public.htm). We processed the Interview, Medical Examination, and Laboratory, and Mortality data using the [SAS code provided by NCHS](https://wwwn.cdc.gov/nchs/nhanes/nhanes3/DataFiles.aspx), SAS University Edition version `9.04.01M5P09132017` on a Jupyter Notebook [@Kluyver_2016; @Perez_2007] server version `5.1.0` running with Python version `3.5.1` [@python_2003] on the Linux [@Torvalds_2001] operating system version `Red Hat 4.4.7-16` (with GNU Compiler Collection version `4.4.7 20120313`).

We modified the SAS code to save the data as comma-separated-value (`.csv`) files, which are available on [FigShare](https://figshare.com/articles/adult_csv/6210263). The SAS code files (`.sas`) and analogous Jupyter Notebook files (`.ipynb`) are available on [GitHub](https://github.com/marskar/nhanes). We then read the `.csv` files into open-source R software [@Rcore_2018] version `3.5` using the `readr` R package [@readr_2017]. R has a vibrant community and a rich ecosystem of software packages. All of the software packages used in this work can be accessed from the Comprehensive R Archive Network (CRAN) [@Hornik_2017] or from GitHub [@Vuorre_2018] using the `devtools` package [@devtools_2018].

Next, we used the `dplyr` R package [@dplyr_2017] to 1) remove all NHANES participant identifiers (`SEQN`) without cause of death (`UCOD_LEADING`) or follow-up time from interview (`PERMTH_INT`) variables, 2) create a cancer mortality variable based on whether the cause of death was "Malignant neoplasms (`C00-C97`)", and 3) join all four datasets together by the participant identifier variable. From the combined dataset, we removed baseline cancer cases (using the interview variables `HAC1N` and `HAC1O`), participants that were missing the relevant NHANES sampling variables (`SDDPSU6`, `SDSTRA6`, and `WTPFQX6`), variables with a time origin other than the date of interview (e.g. `PERMTH_EXM` ), unnecessary NHANES sampling variables, and variables that were based on or similar to the main age variable (such as the age in months, `HSAITMOR`). To create the final processed dataset, we also removed highly correlated variables ($r \geq 0.9$) using the `caret` R package.

Methods to analyze complex survey data using SAS, SPSS, STATA, SUDAAN, [@Siller_2006] and R [@Lumley_2004; @survey_2017] software have been described. From the final dataset, we randomly selected 1 to 50 predictor variables and trained Cox proportional hazards models with the `survey` R package [@Lumley_2004; @survey_2017], which allows for the analysis of complex survey design data using R [@Lumley_2011]. In half of the models, we applied ridge penalties [@Hoerl_1970] to the predictors variables using the `survival` R Package [@Therneau_2000; @survival_2015]. In addition to the predictor variables, the models also included 1) a "survival object" [@Therneau_2000; @survival_2015] created from the event (cancer mortality) and follow-up time variables and 2) a "design object" [@survey_2017] created from the Primary Sampling Unit (`SDDPSU6`), Stratification (`SDSTRA6`) and Weight (`WTPFQX6`) NHANES sampling variables^[The [National Center for Health Statistics (NCHS)](https://www.cdc.gov/nchs/tutorials/NHANES/SurveyDesign/SampleDesign/intro_iii.htm) recommends the application of the provided sampling design variables and sampling weights in all NHANES analyses.].

We then calculated statistics describing the models and the variables they contained and saved these statistics as `.rds` files using the `readr` package [@readr_2017]. We automated the modeling and statistical analyses using the `purrr` R package [@purrr_2017] and GNU Make [@Mecklenburg_2004]. Specifically, the model statistics collected were concordance [@Bozdogan_1987] and Akaike Information Criterion (AIC) [@Gonen_2005] values, while the variable statistics were p-values, hazard ratios, and hazard ratio confidence intervals. We unpacked the model and variable data using the `tidyr` R package [@tidyr_2018].

Next, we selected 3 potential confounder variables representing age (`HSAGEIR`) race/ethnicity (`DMAETHNR`), sex (`HSSEX`) and repeated the modeling and statistical analysis process described above. For the final modeling run, we chose an additional 5 variables (`HAB1`, `HAR1`, `HAQ1`, `HAT2`, and `HAT10`) that appeared with high frequency as highly significant (p-value < 10^-10^) variables in the models we trained earlier. We joined all of the model and variable statistics together, standardized column names using the `stringr` [@stringr_2018] R package, and reordered the variable names according to their counts using the `forcats` [@forcats_2018] R package. To make the final figures, the concordance and AIC values (Figure 1), p-values and hazard ratios (Figure 2) and the number of times each variable appeared in the models (Figure 3) were plotted using the `ggplot2` R package [@gglot2_2009].

## Results

We present the data from thousands of Cox proportional hazards models (n = 3789) we trained on NHANES III data in three iterative steps. The Akaike Information Criterion (AIC) [@Gonen_2005] and concordance values [@Bozdogan_1987] for all models are plotted in Figure 1. To better understand the models created during the first iteration, we divided the Group 1 models into 4 subgroups (1A, 1B, 1C, and 1D) based on their AIC and concordance values. The models from the first iteration (Figure 1; Groups 1A-D; green, cyan, blue, purple) were fully randomized in terms of the predictor variables that were included, while the next two iterations (Figure 1; Groups 2-3; orange, red) consisted of models that started with 3 and 8 non-randomized variables, respectively, before the addition of randomly chosen variables. The 3 variables included in the second and third iteration were age, sex, and race/ethnicity, whereas the final iteration contained an additional 5 variables, which appeared frequently as highly significant (p < 10^-10^) variables in the previous iterations. In all cases, the models contained up to 50 predictor variables.

The models from the third iteration (Figure 1; Group 3; red) had the highest concordance values overall, indicating that the addition of the 8 non-randomized predictor variables led to higher discriminatory power between low and high-risk individuals. The gains in concordance seem to be largely due to the addition of the age, sex, and race/ethnicity variables as the concordance values we obtained from the second (Figure 1; Group 2; orange) and third (Figure 1; Group 3; red) iterations were similar. Interestingly, models from the third iteration all had concordance values of 84 or higher (Figure 1; black horizontal line), while the range of AIC values was roughly the same in all three groups of models (Figure 1). This finding suggests that while concordance can differentiate between models from the three iterations, AIC by itself is unable to make this distinction.

![**Cancer Mortality Risk Prediction Models Trained on NHANES III Data.** Each point in the scatter plot represents a Cox proportional hazards model (n=3789) trained on NHANES III data. The sizes of the points are relative to the number of variables (maximum = 50) in each the model, while the shapes correspond to whether ridge penalties were applied (triangle) or not (circle). The colors of points distinguish between models that had 0 (Groups 1A-D; green, cyan, blue, purple), 3 (Group 2; orange) or 8 (Group 3; red) non-randomized predictor variables. Additionally, Group 1 models are further color coded by quadrants based on concordance and Akaike Information Criterion (AIC) values as follows: high-concordance and low-AIC (Group 1A; green), high-concordance and high-AIC (Group 1B; cyan), low-concordance and low-AIC (Group 1C; blue), low-concordance and high-AIC (Group 1D; purple). All Group 3 models have concordance values of 84 or higher (black horizontal line).](img/1-quad-final100dpi.png){width=100%}

The addition of a metric like AIC is important, because it serves to provide a balance of goodness-of-fit and model complexity. Concordance, unlike AIC, does not take into account the complexity of a model. As follows, larger models tended to have higher concordance values, but also higher AIC values. We applied ridge penalties [@Hoerl_1970], also known as L2 regularization [@Ng_2004], to half of the models from all three iterations, and noted that ridge penalization controls this increase in AIC as models become larger (Figure 1). The relationship between model size and concordance appears to plateau as concordance increases (Figure 1), which suggests that the models are reaching the limit of what is possible with the available 243 variables. Though there is almost certainly another combination of variables that would lead to further improvements in concordance, our approach allowed us to generate a series of models that perform well without the need to test every possible combination of the variables.

To choose variables to be included as non-randomized variables, we consulted the NHANES variables descriptions available on the [Centers for Disease Control (CDC) - National Center for Health Statistics (NCHS) website](https://wwwn.cdc.gov/nchs/nhanes/nhanes3/DataFiles.aspx) and for the third iteration in particular we only considered variables that had p-values lower than 10^-10^ (Figure 2; black horizontal line). It would be possible to introduce a threshold for hazard ratios (Figure 2; x-axis), but this approach would tend to select models without ridge penalties. The coefficients in ridge penalized models are shrunk based on the penalty that is applied, which in this case means that ridge penalized models have hazard ratios closer to zero. While the significance and hazard ratios of variables depend on the other variables in the model, our method allows us to survey the landscape of p-values and hazard ratios of variables in the models trained (Figure 2).

![**Predictor Variables from NHANES III Lethal Cancer Risk Prediction Models.** Each point in the volcano plot represents a predictor variable (n = 98787) from a Cox proportional hazards model (n = 3789) trained on NHANES III data. Variables are considered to be highly significant when their negative log10 p-values (y-axis) are above 10 (black horizontal line), regardless of their log2 hazard ratios (x-axis). The shapes of points correspond to whether ridge penalties were applied (triangle) or not (circle). The colors of points describe the model each variable come from as in Figure 1.](img/2-volcano-final100dpi.png){width=100%}

The names, median hazard ratios, counts and descriptions of the ten most frequent highly significant variables are summarized in Table 1. The variable that appeared most frequently as highly significant across the all of the models was age (Figure 3; `HSAGEIR`). When focusing on the Group 1 models (Figure 3; Groups 1A-D; green, cyan, blue, purple), the most frequent highly significant variable was an interview question regarding dental health (Figure 3; `HAQ1`). The other top 10 high-frequency highly significant variables were race/ethnicity (`DMAETHNR`), lifetime consumption of more than 100 cigarettes (`HAR1`), 3 variables related to physical activity (`HAT2`, `HAT10`, `HAT16`), and 3 variables that may be associated to aging (`HAB7`, `HAK9`, and `HAP2`). Interestingly, one of the variables, "In the past 12 months, how many times were you in a nursing home?" (`HAB7`), was present as a highly significant variable only in the second and third groups.

![The Ten Most Frequent, Highly Significant Predictor Variables. Each row in the table represents one of the ten predictor variables that appeared most frequently as highly significant (p < 10^-10^) variables in the Cox models (n = 3789) trained on NHANES III data. The median hazard ratio (HR) and count (n) statistics are calculated on highly significant variables only. The variables descriptions are based on the [documentation on the NHANES III website](https://wwwn.cdc.gov/nchs/nhanes/nhanes3/DataFiles.aspx).](img/3-varbar-final100dpi.png){width=100%}

|Name     | Median HR|    n|  Description |
|:--------|---------:|----:|:-------------|
|HSAGEIR  |      1.04| 2016| Age in Years |
|HAQ1     |      1.07| 1415| How would you describe the condition of your natural teeth (excellent, very good, good, fair or poor)? |
|HAT2     |      1.50|  384| In the past month, did you jog or run? |
|HAT16    |      1.67|  256| In the past month, did you lift weights? |
|HAB7     |      0.99|  228| In the past 12 months, how many times were you in a nursing home? |
|DMAETHNR |      1.14|  224| Race/Ethnicity |
|HAK9     |      1.23|  216| How many times per night do you usually get up to urinate? |
|HAP2     |      0.81|  179| Do you use glasses, contacts, or both? |
|HAT10    |      1.43|  119| In the past month, did you do other dancing? |
|HAR1     |      0.63|   96| Have you smoked at least 100 cigarettes during your entire life? |

: Description of Highly Significant Variables that Appeared Most Frequently

The ranks of variables shown is Table 1 are determined by counts from all 3789 models, and thus are heavily influenced by the fact that some variables are included in all Group 2 (`HSAGEIR`, `DMAETHNR`, and `HSSEX`) and 3 models (`HSAGEIR`, `DMAETHNR`, `HSSEX`, `HAB1`, `HAR1`, `HAQ1`, `HAT2`, and `HAT10`). Table 1 therefore serves as a summary of all three iterations of modeling and statistical analysis. Rather than selecting the variables with the lowest p-values or highest hazard ratios, we chose to follow a strategy that counts the number of times a variable's significance crosses a p-value threshold. This type of frequency-based ranking of variables can be used to both guide future variable selection decisions and assess previous steps in the model building process.

## Discussion

To obtain a better understanding of how variables for cancer risk prediction models can be selected, we utilized an iterative strategy to explore the variables in the NHANES III dataset. As part of this strategy, we randomly generated a large number of Cox proportional hazards models to guide the training of new models with fewer randomly chosen variables in future iterations. This method is akin to forward subset selection [@Kohavi_1997] in that models are built up variable by variable, but differs in that models are not assessed with the addition of each new variable. In fact, the method we employed does not take model performance metrics into consideration when selecting variables. To inform variable selection in subsequent iterations, we instead focused on the frequency with which variables had p-values below 10^-10^. 

The types of variables that were ranked highest in our present analysis (age, race/ethnicity, smoking and physical activity) are all already known to be strongly associated with cancer death. While obtaining the expected result serves to confirm the validity of our method, the main objective of this work is to provide insight that will lead to the identification of new cancer risk factors. To this end, our method will have to be refined to detect variables that weakly contribute to cancer risk or whose contribution is context-specific. In essence, our current approach aggregates information from across many models into a single statistic per variable. Variable selection decisions could be informed by another or a combination of other statistics.

For example, the significance threshold (p < 10^-10^) we put in place was arbitrary and our method could be used with a different threshold value, a different metric or a combination of different thresholds. For example, variables could be selected based on the number of times the absolute value of their hazard ratio crosses a certain threshold. Model statistics could also be employed for thresholds as the concordance, Akaike Information Criterion (AIC) and other model performance metrics remain associated with variables throughout all steps in the process.

In addition to changing the variable selection threshold, the method described here could be adapted to use regularization techniques other than ridge regression [@Hoerl_1970] and models other than Cox proportional hazards models. In terms of regularization, survival analyses can be done with lasso [@Tibshirani_1997] penalties or a combination of ridge and lasso penalties, which is known as Elastic Net [@Zou_2005]. As for possible modeling algorithms to explore in the future include tree-based models such as survival tree [@rpart_2017], survival random forest models [@Ishwaran_2008]. Tree-based models are easy to interpret and allow for the quantification of the proportion of variance explained by variables included in the model. Another statistical method called boosting, for example `XGBoost` [@Chen_2016], can be used to compute F-scores representing the importance of each variable.

Regardless of the algorithms or thresholds used, the final result of our approach is a new dataset of statistics that describe models and variables across all iterations. This new dataset could be merged with text data, such as the [NHANES III variable descriptions provided by the National Center for Health Statistics](https://wwwn.cdc.gov/nchs/nhanes/nhanes3/DataFiles.aspx), and employ Natural Language Processing (NLP) [@Chowdhury_2003] techniques to add further the information related to the models and variables in the dataset. For example, NLP techniques could be used to classify variables into categories, such as physical activity or nutrition, based on their descriptions. All of this information could then be combined with domain knowledge to steer the variable selection process.


We demonstrated the ability of our method to generate models that predict lethal cancer risk in the NHANES III dataset with high accuracy ($concordance \geq 84$). It remains to be seen, whether our approach could be generalizable to other studies and other outcomes. NHANES III does not include cancer-type-specific mortality data, but other studies, such as the Atherosclerosis Risk in Communities study (ARIC) [@ARIC_1989; @Joshu_2017], may provide the opportunity to generate and assess models that predict mortality or incidence of a specific type of cancer. As a continuation of this project, we expand the methods described here into a general methodology that can be applied beyond NHANES III to other large, high-dimensional cohort studies. In addition to generalization to other studies, future work on this project will include the creation of a software package that encapsulates all of the relevant code and a graphical user interface that facilitates data exploration, model parameter modification and variable selection.

## Connections to MPH Goals Analysis

My MPH experience at Johns Hopkins has been absolutely transformative. Though I started my MPH studies with a strong background in science, I had no experience with public health or population science. Similarly, while I was comfortable with R programming, I did not know the first thing about survey data, let along how to conduct complex survey analyses in R. In my Goals Analysis Plan, I outlined my goal of broadening my horizons in three areas: substantive (domain) knowledge, statistics (mathematics), and technical (programming) skills. , is This NHANES III Research Report Capstone Project is testament to the skills I honed and the knowledge I gained over the past year and also perfectly aligns with my interests. I look forward to building upon this capstone project as continue my career in cancer prevention research.

In terms of substantive knowledge, I wanted to focus on cancer epidemiology

## References


<!--chapter:end:index.Rmd-->

<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# R Markdown Basics {#rmd-basics}

Here is a brief introduction into using _R Markdown_. _Markdown_ is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. _R Markdown_ provides the flexibility of _Markdown_ with the implementation of **R** input and output.  For more details on using _R Markdown_ see <http://rmarkdown.rstudio.com>.  

Be careful with your spacing in _Markdown_ documents.  While whitespace largely is ignored, it does at times give _Markdown_ signals as to how to proceed.  As a habit, try to keep everything left aligned whenever possible, especially as you type a new paragraph.  In other words, there is no need to indent basic text in the Rmd document (in fact, it might cause your text to do funny things if you do).

## Lists

It's easy to create a list.  It can be unordered like

* Item 1
* Item 2

or it can be ordered like

1. Item 1
4. Item 2

Notice that I intentionally mislabeled Item 2 as number 4.  _Markdown_ automatically figures this out!  You can put any numbers in the list and it will create the list.  Check it out below.

To create a sublist, just indent the values a bit (at least four spaces or a tab).  (Here's one case where indentation is key!)

1. Item 1
1. Item 2
1. Item 3
    - Item 3a
    - Item 3b

## Line breaks

Make sure to add white space between lines if you'd like to start a new paragraph.  Look at what happens below in the outputted document if you don't:

Here is the first sentence.  Here is another sentence.  Here is the last sentence to end the paragraph.
This should be a new paragraph.

*Now for the correct way:* 

Here is the first sentence.  Here is another sentence.  Here is the last sentence to end the paragraph.

This should be a new paragraph.

## R chunks

When you click the **Knit** button above a document will be generated that includes both content as well as the output of any embedded **R** code chunks within the document. You can embed an **R** code chunk like this (`cars` is a built-in **R** dataset):

```{r cars}
summary(cars)
```

## Inline code

If you'd like to put the results of your analysis directly into your discussion, add inline code like this:

> The `cos` of $2 \pi$ is `r cos(2*pi)`. 

Another example would be the direct calculation of the standard deviation:

> The standard deviation of `speed` in `cars` is `r sd(cars$speed)`.

One last neat feature is the use of the `ifelse` conditional statement which can be used to output text depending on the result of an **R** calculation:

> `r ifelse(sd(cars$speed) < 6, "The standard deviation is less than 6.", "The standard deviation is equal to or greater than 6.")`

Note the use of `>` here, which signifies a quotation environment that will be indented.

As you see with `$2 \pi$` above, mathematics can be added by surrounding the mathematical text with dollar signs.  More examples of this are in [Mathematics and Science] if you uncomment the code in [Math].  

## Including plots

You can also embed plots.  For example, here is a way to use the base **R** graphics package to produce a plot using the built-in `pressure` dataset:

```{r pressure, echo=FALSE, cache=TRUE}
plot(pressure)
```

Note that the `echo=FALSE` parameter was added to the code chunk to prevent printing of the **R** code that generated the plot.  There are plenty of other ways to add chunk options.  More information is available at <http://yihui.name/knitr/options/>.  

Another useful chunk option is the setting of `cache=TRUE` as you see here.  If document rendering becomes time consuming due to long computations or plots that are expensive to generate you can use knitr caching to improve performance.  Later in this file, you'll see a way to reference plots created in **R** or external figures.

## Loading and exploring data

Included in this template is a file called `flights.csv`.  This file includes a subset of the larger dataset of information about all flights that departed from Seattle and Portland in 2014.  More information about this dataset and its **R** package is available at <http://github.com/ismayc/pnwflights14>.  This subset includes only Portland flights and only rows that were complete with no missing values.  Merges were also done with the `airports` and `airlines` data sets in the `pnwflights14` package to get more descriptive airport and airline names.

We can load in this data set using the following command:

```{r load_data}
flights <- read.csv("data/flights.csv")
```

The data is now stored in the data frame called `flights` in **R**.  To get a better feel for the variables included in this dataset we can use a variety of functions.  Here we can see the dimensions (rows by columns) and also the names of the columns.

```{r str}
dim(flights)
names(flights)
```

Another good idea is to take a look at the dataset in table form.  With this dataset having more than 50,000 rows, we won't explicitly show the results of the command here.  I recommend you enter the command into the Console **_after_** you have run the **R** chunks above to load the data into **R**.

```{r view_flights, eval=FALSE}
View(flights)
```

While not required, it is highly recommended you use the `dplyr` package to manipulate and summarize your data set as needed.  It uses a syntax that is easy to understand using chaining operations.  Below I've created a few examples of using `dplyr` to get information about the Portland flights in 2014.  You will also see the use of the `ggplot2` package, which produces beautiful, high-quality academic visuals.

We begin by checking to ensure that needed packages are installed and then we load them into our current working environment:

```{r load_pkgs, message=FALSE}
# List of packages required for this analysis
pkg <- c("dplyr", "ggplot2", "knitr", "bookdown", "devtools")
# Check if packages are not installed and assign the
# names of the packages not installed to the variable new.pkg
new.pkg <- pkg[!(pkg %in% installed.packages())]
# If there are any packages in the list that aren't installed,
# install them
if (length(new.pkg))
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
# Load packages (thesisdown will load all of the packages as well)
library(thesisdown)
```

\clearpage

The example we show here does the following:

- Selects only the `carrier_name` and `arr_delay` from the `flights` dataset and then assigns this subset to a new variable called `flights2`. 

- Using `flights2`, we determine the largest arrival delay for each of the carriers.

```{r max_delays}
flights2 <- flights %>% 
  select(carrier_name, arr_delay)
max_delays <- flights2 %>% 
  group_by(carrier_name) %>%
  summarize(max_arr_delay = max(arr_delay, na.rm = TRUE))
```

A useful function in the `knitr` package for making nice tables in _R Markdown_ is called `kable`.  It is much easier to use than manually entering values into a table by copying and pasting values into Excel or LaTeX.  This again goes to show how nice reproducible documents can be! (Note the use of `results="asis"`, which will produce the table instead of the code to create the table.)  The `caption.short` argument is used to include a shorter title to appear in the List of Tables.

```{r maxdelays, results="asis"}
kable(max_delays, 
      col.names = c("Airline", "Max Arrival Delay"),
      caption = "Maximum Delays by Airline",
      caption.short = "Max Delays by Airline",
      longtable = TRUE,
      booktabs = TRUE)
```

The last two options make the table a little easier-to-read.

We can further look into the properties of the largest value here for American Airlines Inc.  To do so, we can isolate the row corresponding to the arrival delay of 1539 minutes for American in our original `flights` dataset.


```{r max_props}
flights %>% filter(arr_delay == 1539, 
                  carrier_name == "American Airlines Inc.") %>%
  select(-c(month, day, carrier, dest_name, hour, 
            minute, carrier_name, arr_delay))
```

We see that the flight occurred on March 3rd and departed a little after 2 PM on its way to Dallas/Fort Worth.  Lastly, we show how we can visualize the arrival delay of all departing flights from Portland on March 3rd against time of departure.

```{r march3plot, fig.height=3, fig.width=6}
flights %>% filter(month == 3, day == 3) %>%
  ggplot(aes(x = dep_time, y = arr_delay)) + geom_point()
```

## Additional resources

- _Markdown_ Cheatsheet - <https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet>

- _R Markdown_ Reference Guide - <https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf>

- Introduction to `dplyr` - <https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html>

- `ggplot2` Documentation - <http://docs.ggplot2.org/current/>

<!--chapter:end:01-chap1.Rmd-->

# Mathematics and Science {#math-sci}

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

## Math

\TeX\ is the best way to typeset mathematics. Donald Knuth designed \TeX\ when he got frustrated at how long it was taking the typesetters to finish his book, which contained a lot of mathematics.  One nice feature of _R Markdown_ is its ability to read LaTeX code directly.

If you are doing a thesis that will involve lots of math, you will want to read the following section which has been commented out. If you're not going to use math, skip over or delete this next commented section.


<!-- MATH and PHYSICS majors: Uncomment the following section -->
<!--
$$\sum_{j=1}^n (\delta\theta_j)^2 \leq {{\beta_i^2}\over{\delta_i^2 + \rho_i^2}}
\left[ 2\rho_i^2 + {\delta_i^2\beta_i^2\over{\delta_i^2 + \rho_i^2}} \right] \equiv \omega_i^2
$$

From Informational Dynamics, we have the following (Dave Braden):

After _n_ such encounters the posterior density for $\theta$ is

$$
\pi(\theta|X_1< y_1,\dots,X_n<y_n) \varpropto \pi(\theta) \prod_{i=1}^n\int_{-\infty}^{y_i}
   \exp\left(-{(x-\theta)^2\over{2\sigma^2}}\right)\ dx
$$

Another equation:

$$\det\left|\,\begin{matrix}%
c_0&c_1\hfill&c_2\hfill&\ldots&c_n\hfill\cr
c_1&c_2\hfill&c_3\hfill&\ldots&c_{n+1}\hfill\cr
c_2&c_3\hfill&c_4\hfill&\ldots&c_{n+2}\hfill\cr
\,\vdots\hfill&\,\vdots\hfill&
  \,\vdots\hfill&&\,\vdots\hfill\cr
c_n&c_{n+1}\hfill&c_{n+2}\hfill&\ldots&c_{2n}\hfill\cr
\end{matrix}\right|>0$$


Lapidus and Pindar, Numerical Solution of Partial Differential Equations in Science and
Engineering.  Page 54

$$
\int_t\left\{\sum_{j=1}^3 T_j \left({d\phi_j\over dt}+k\phi_j\right)-kT_e\right\}w_i(t)\ dt=0,
   \qquad\quad i=1,2,3.
$$

L\&P  Galerkin method weighting functions.  Page 55

$$
\sum_{j=1}^3 T_j\int_0^1\left\{{d\phi_j\over dt} + k\phi_j\right\} \phi_i\ dt
   = \int_{0}^1k\,T_e\phi_idt, \qquad i=1,2,3 $$

Another L\&P (p145)

$$
\int_{-1}^1\!\int_{-1}^1\!\int_{-1}^1 f\big(\xi,\eta,\zeta\big)
   = \sum_{k=1}^n\sum_{j=1}^n\sum_{i=1}^n w_i w_j w_k f\big( \xi,\eta,\zeta\big).
$$

Another L\&P (p126)

$$
\int_{A_e} (\,\cdot\,) dx dy = \int_{-1}^1\!\int_{-1}^1 (\,\cdot\,) \det[J] d\xi d\eta.
$$
-->

## Chemistry 101: Symbols

Chemical formulas will look best if they are not italicized. Get around math mode's automatic italicizing in LaTeX by using the argument `$\mathrm{formula here}$`, with your formula inside the curly brackets.  (Notice the use of the backticks here which enclose text that acts as code.)

So, $\mathrm{Fe_2^{2+}Cr_2O_4}$ is written `$\mathrm{Fe_2^{2+}Cr_2O_4}$`.

<!--
The \noindent command below does what you'd expect:  it forces the current line/paragraph to not indent. This was done here to match the format of the LaTeX thesis PDF.
-->

\noindent Exponent or Superscript: $\mathrm{O^-}$

\noindent Subscript: $\mathrm{CH_4}$

To stack numbers or letters as in $\mathrm{Fe_2^{2+}}$, the subscript is defined first, and then the superscript is defined.

\noindent Bullet: CuCl $\bullet$ $\mathrm{7H_{2}O}$


\noindent Delta: $\Delta$

\noindent Reaction Arrows: $\longrightarrow$ or  $\xrightarrow{solution}$

\noindent Resonance Arrows: $\leftrightarrow$

\noindent Reversible Reaction Arrows: $\rightleftharpoons$

### Typesetting reactions

You may wish to put your reaction in an equation environment, which means that LaTeX will place the reaction where it fits and will number the equations for you. 

\begin{equation}
  \mathrm{C_6H_{12}O_6  + 6O_2} \longrightarrow \mathrm{6CO_2 + 6H_2O}
  (\#eq:reaction)
\end{equation}

We can reference this combustion of glucose reaction via Equation \@ref(eq:reaction).

### Other examples of reactions

$\mathrm{NH_4Cl_{(s)}}$ $\rightleftharpoons$ $\mathrm{NH_{3(g)}+HCl_{(g)}}$

\noindent $\mathrm{MeCH_2Br + Mg}$ $\xrightarrow[below]{above}$ $\mathrm{MeCH_2\bullet Mg \bullet Br}$

## Physics

Many of the symbols you will need can be found on the math page <http://web.reed.edu/cis/help/latex/math.html> and the Comprehensive LaTeX Symbol Guide (<http://mirror.utexas.edu/ctan/info/symbols/comprehensive/symbols-letter.pdf>).

## Biology

You will probably find the resources at <http://www.lecb.ncifcrf.gov/~toms/latex.html> helpful, particularly the links to bsts for various journals. You may also be interested in TeXShade for nucleotide typesetting (<http://homepages.uni-tuebingen.de/beitz/txe.html>).  Be sure to read the proceeding chapter on graphics and tables.


<!--chapter:end:02-chap2.Rmd-->

```{r include_packages_2, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("bookdown", repos = "http://cran.rstudio.com")
if(!require(thesisdown)){
  library(devtools)
  devtools::install_github("ismayc/thesisdown")
  }
library(thesisdown)
flights <- read.csv("data/flights.csv")
```


# Tables, Graphics, References, and Labels {#ref-labels}

## Tables

In addition to the tables that can be automatically generated from a data frame in **R** that you saw in [R Markdown Basics] using the `kable` function, you can also create tables using _pandoc_. (More information is available at <http://pandoc.org/README.html#tables>.)  This might be useful if you don't have values specifically stored in **R**, but you'd like to display them in table form.  Below is an example.  Pay careful attention to the alignment in the table and hyphens to create the rows and columns.

----------------------------------------------------------------------------------
  Factors                    Correlation between Parents & Child      Inherited
------------------------- ----------------------------------------- --------------
  Education                                -0.49                         Yes
  
  Socio-Economic Status                     0.28                        Slight   
  
  Income                                    0.08                          No
  
  Family Size                               0.18                        Slight
  
  Occupational Prestige                     0.21                        Slight
------------------------- ----------------------------------------- --------------
Table: (\#tab:inher) Correlation of Inheritance Factors for Parents and Child 

We can also create a link to the table by doing the following: Table \@ref(tab:inher).  If you go back to [Loading and exploring data] and look at the `kable` table, we can create a reference to this max delays table too: Table \@ref(tab:maxdelays). The addition of the `(\#tab:inher)` option to the end of the table caption allows us to then make a reference to Table `\@ref(tab:label)`. Note that this reference could appear anywhere throughout the document after the table has appeared.  

<!-- We will next explore ways to create this label-ref link using figures. -->

\clearpage

<!-- clearpage ends the page, and also dumps out all floats.
  Floats are things like tables and figures. -->


## Figures

If your thesis has a lot of figures, _R Markdown_ might behave better for you than that other word processor.  One perk is that it will automatically number the figures accordingly in each chapter.    You'll also be able to create a label for each figure, add a caption, and then reference the figure in a way similar to what we saw with tables earlier.  If you label your figures, you can move the figures around and _R Markdown_ will automatically adjust the numbering for you.  No need for you to remember!  So that you don't have to get too far into LaTeX to do this, a couple **R** functions have been created for you to assist.  You'll see their use below.

<!--
One thing that may be annoying is the way _R Markdown_ handles "floats" like tables and figures (it's really \LaTeX's fault). \LaTeX\ will try to find the best place to put your object based on the text around it and until you're really, truly done writing you should just leave it where it lies. There are some optional arguments specified in the options parameter of the `label` function.  If you need to shift your figure around, it might be good to look here on tweaking the options argument:  <https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions>

If you need a graphic or tabular material to be part of the text, you can just put it inline. If you need it to appear in the list of figures or tables, it should be placed in a code chunk.
-->


In the **R** chunk below, we will load in a picture stored as `reed.jpg` in our main directory.  We then give it the caption of "Reed logo", the label of "reedlogo", and specify that this is a figure.  Make note of the different **R** chunk options that are given in the R Markdown file (not shown in the knitted document).

```{r reedlogo, fig.cap="Reed logo"}
include_graphics(path = "figure/reed.jpg")
```

Here is a reference to the Reed logo: Figure \@ref(fig:reedlogo).  Note the use of the `fig:` code here.  By naming the **R** chunk that contains the figure, we can then reference that figure later as done in the first sentence here.  We can also specify the caption for the figure via the R chunk option `fig.cap`.

\clearpage 

<!-- starts a new page and stops trying to place floats such as tables and figures -->

Below we will investigate how to save the output of an **R** plot and label it in a way similar to that done above.  Recall the `flights` dataset from Chapter \@ref(rmd-basics).  (Note that we've shown a different way to reference a section or chapter here.)  We will next explore a bar graph with the mean flight departure delays by airline from Portland for 2014.  Note also the use of the `scale` parameter which is discussed on the next page.

```{r delaysboxplot, warnings=FALSE, messages=FALSE, fig.cap="Mean Delays by Airline", fig.width=6}
flights %>% group_by(carrier) %>%
  summarize(mean_dep_delay = mean(dep_delay)) %>%
  ggplot(aes(x = carrier, y = mean_dep_delay)) +
  geom_bar(position = "identity", stat = "identity", fill = "red")
```

Here is a reference to this image: Figure \@ref(fig:delaysboxplot).

A table linking these carrier codes to airline names is available at <https://github.com/ismayc/pnwflights14/blob/master/data/airlines.csv>.

\clearpage

Next, we will explore the use of the `out.extra` chunk option, which can be used to shrink or expand an image loaded from a file by specifying `"scale= "`. Here we use the mathematical graph stored in the "subdivision.pdf" file.

```{r subd, results="asis", echo=FALSE, fig.cap="Subdiv. graph", out.extra="scale=0.75"}
include_graphics("figure/subdivision.pdf")
```

Here is a reference to this image: Figure \@ref(fig:subd).  Note that `echo=FALSE` is specified so that the **R** code is hidden in the document.

**More Figure Stuff**

Lastly, we will explore how to rotate and enlarge figures using the `out.extra` chunk option.  (Currently this only works in the PDF version of the book.)

```{r subd2, results="asis", echo=FALSE, out.extra="angle=180, scale=1.1", fig.cap="A Larger Figure, Flipped Upside Down"}
include_graphics("figure/subdivision.pdf")
```

As another example, here is a reference: Figure \@ref(fig:subd2).  

## Footnotes and Endnotes

You might want to footnote something. ^[footnote text] The footnote will be in a smaller font and placed appropriately. Endnotes work in much the same way. More information can be found about both on the CUS site or feel free to reach out to <data@reed.edu>.

## Bibliographies

Of course you will need to cite things, and you will probably accumulate an armful of sources. There are a variety of tools available for creating a bibliography database (stored with the .bib extension).  In addition to BibTeX suggested below, you may want to consider using the free and easy-to-use tool called Zotero.  The Reed librarians have created Zotero documentation at <http://libguides.reed.edu/citation/zotero>.  In addition, a tutorial is available from Middlebury College at <http://sites.middlebury.edu/zoteromiddlebury/>.

_R Markdown_ uses _pandoc_ (<http://pandoc.org/>) to build its bibliographies.  One nice caveat of this is that you won't have to do a second compile to load in references as standard LaTeX requires. To cite references in your thesis (after creating your bibliography database), place the reference name inside square brackets and precede it by the "at" symbol.  For example, here's a reference to a book about worrying:  [@Molina1994].  This `Molina1994` entry appears in a file called `thesis.bib` in the `bib` folder.  This bibliography database file was created by a program called BibTeX.  You can call this file something else if you like (look at the YAML header in the main .Rmd file) and, by default, is to placed in the `bib` folder.  

For more information about BibTeX and bibliographies, see our CUS site (<http://web.reed.edu/cis/help/latex/index.html>)^[@reedweb2007]. There are three pages on this topic:  _bibtex_ (which talks about using BibTeX, at <http://web.reed.edu/cis/help/latex/bibtex.html>), _bibtexstyles_ (about how to find and use the bibliography style that best suits your needs, at <http://web.reed.edu/cis/help/latex/bibtexstyles.html>) and _bibman_ (which covers how to make and maintain a bibliography by hand, without BibTeX, at <http://web.reed.edu/cis/help/latex/bibman.html>). The last page will not be useful unless you have only a few sources.

If you look at the YAML header at the top of the main .Rmd file you can see that we can specify the style of the bibliography by referencing the appropriate csl file.  You can download a variety of different style files at <https://www.zotero.org/styles>.  Make sure to download the file into the csl folder.

**Tips for Bibliographies**

- Like with thesis formatting, the sooner you start compiling your bibliography for something as large as thesis, the better. Typing in source after source is mind-numbing enough; do you really want to do it for hours on end in late April? Think of it as procrastination.
- The cite key (a citation's label) needs to be unique from the other entries.
- When you have more than one author or editor, you need to separate each author's name by the word "and" e.g. `Author = {Noble, Sam and Youngberg, Jessica},`.
- Bibliographies made using BibTeX (whether manually or using a manager) accept LaTeX markup, so you can italicize and add symbols as necessary.
- To force capitalization in an article title or where all lowercase is generally used, bracket the capital letter in curly braces.
- You can add a Reed Thesis citation^[@noble2002] option. The best way to do this is to use the phdthesis type of citation, and use the optional "type" field to enter "Reed thesis" or "Undergraduate thesis." 

## Anything else?

If you'd like to see examples of other things in this template, please contact the Data @ Reed team (email <data@reed.edu>) with your suggestions. We love to see people using _R Markdown_ for their theses, and are happy to help.


<!--chapter:end:03-chap3.Rmd-->

# Conclusion {-}

If we don't want Conclusion to have a chapter number next to it, we can add the `{-}` attribute.

**More info**

And here's some other random info: the first paragraph after a chapter title or section head _shouldn't be_ indented, because indents are to tell the reader that you're starting a new paragraph. Since that's obvious after a chapter or section title, proper typesetting doesn't add an indent there.


<!--chapter:end:04-conclusion.Rmd-->

`r if(knitr:::is_latex_output()) '\\appendix'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 

<!--
If you feel it necessary to include an appendix, it goes here.
-->


# The First Appendix

This first appendix includes all of the R chunks of code that were hidden throughout the document (using the `include = FALSE` chunk tag) to help with readibility and/or setup.

**In the main Rmd file**

```{r ref.label='include_packages', results='hide', echo = TRUE}
```

**In Chapter \@ref(ref-labels):**

```{r ref.label='include_packages_2', results='hide', echo = TRUE}
```

# The Second Appendix, for Fun

<!--chapter:end:05-appendix.Rmd-->

<!--
The bib chunk below must go last in this document according to how R Markdown renders.  More info is at http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
-->

\backmatter

<!-- 
If you'd like to change the name of the bibliography to something else,
delete "References" and replace it.
-->

# References {-}
<!--
This manually sets the header for this unnumbered chapter.
-->
\markboth{References}{References}
<!--
To remove the indentation of the first entry.
-->
\noindent

<!--
To create a hanging indent and spacing between entries.  These three lines may need to be removed for styles that don't require the hanging indent.
-->

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}


<!--
This is just for testing with more citations for the bibliography at the end.  Add other entries into the list here if you'd like them to appear in the bibliography even if they weren't explicitly cited in the document.
-->

---
nocite: | 
  @angel2000, @angel2001, @angel2002a
...

<!--chapter:end:99-references.Rmd-->

